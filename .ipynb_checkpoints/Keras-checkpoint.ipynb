{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Knowledge' 'Knowledge' 'Knowledge' 'Knowledge' 'Knowledge' 'Knowledge'\n",
      " 'Knowledge' 'Knowledge' 'Knowledge' 'Knowledge' 'Knowledge' 'Knowledge'\n",
      " 'Knowledge' 'Knowledge' 'Knowledge' 'Knowledge' 'Knowledge' 'Knowledge'\n",
      " 'Knowledge' 'Knowledge' 'Knowledge' 'Knowledge' 'Knowledge' 'Knowledge'\n",
      " 'Knowledge' 'Knowledge' 'Comprehension' 'Comprehension' 'Comprehension'\n",
      " 'Comprehension' 'Comprehension' 'Comprehension' 'Comprehension'\n",
      " 'Comprehension' 'Comprehension' 'Comprehension' 'Comprehension'\n",
      " 'Comprehension' 'Comprehension' 'Comprehension' 'Comprehension'\n",
      " 'Comprehension' 'Comprehension' 'Comprehension' 'Comprehension'\n",
      " 'Comprehension' 'Comprehension' 'Comprehension' 'Comprehension'\n",
      " 'Application' 'Application' 'Application' 'Application' 'Application'\n",
      " 'Application' 'Application' 'Application' 'Application' 'Application'\n",
      " 'Application' 'Application' 'Application' 'Application' 'Application'\n",
      " 'Analysis' 'Analysis' 'Analysis' 'Analysis' 'Analysis' 'Analysis'\n",
      " 'Analysis' 'Analysis' 'Analysis' 'Analysis' 'Analysis' 'Analysis'\n",
      " 'Analysis' 'Analysis' 'Analysis' 'Analysis' 'Analysis' 'Analysis'\n",
      " 'Analysis' 'Analysis' 'Analysis' 'Analysis' 'Analysis' 'Synthesis'\n",
      " 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis'\n",
      " 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis'\n",
      " 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis'\n",
      " 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis'\n",
      " 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis' 'Synthesis' 'Evaluation'\n",
      " 'Evaluation' 'Evaluation' 'Evaluation' 'Evaluation' 'Evaluation'\n",
      " 'Evaluation' 'Evaluation' 'Evaluation' 'Evaluation' 'Evaluation'\n",
      " 'Evaluation' 'Evaluation' 'Evaluation' 'Evaluation' 'Evaluation'\n",
      " 'Evaluation' 'Evaluation' 'Evaluation' 'Evaluation' 'Evaluation'\n",
      " 'Evaluation' 'Evaluation' 'Evaluation']\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# Load dataset\n",
    "url = \"/home/shobhit/Machine_Learning/sentences_600.csv\"\n",
    "url1 = \"/home/shobhit/Machine_Learning/train.csv\"\n",
    "names = ['Sentence', 'Label']\n",
    "dataset = read_csv(url, names=names)\n",
    "dataset1 = read_csv(url1, names=names)\n",
    "array = dataset.values\n",
    "array1 = dataset1.values\n",
    "X = array[:,0]\n",
    "y = array[:,1]\n",
    "X1=array1[:,0]\n",
    "Y1=array1[:,1]\n",
    "print(Y1)\n",
    "d={\"Knowledge\":0,\"Comprehension\":1,\"Application\":2,\"Analysis\":3,\"Synthesis\":4,\"Evaluation\":5}\n",
    "for i in range(len(y)):\n",
    "    y[i]=d[y[i]]\n",
    "\n",
    "for i in range(len(Y1)):\n",
    "    Y1[i]=d[Y1[i]]\n",
    "\n",
    "sentences_train, sentences_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, shuffle=True, random_state=1)\n",
    "sentences_train1, sentences_validation1, Y_train1, Y_validation1 = train_test_split(X1, Y1, test_size=0.1, shuffle=True, random_state=1)\n",
    "sentences_train1=np.append(sentences_train1,sentences_validation1)\n",
    "Y_test=np.append(Y_train1,Y_validation1)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)\n",
    "# print(vectorizer.get_feature_names());\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_validation  = vectorizer.transform(sentences_validation)\n",
    "X_test=vectorizer.transform(sentences_train1)\n",
    "# print(X_train[0].toarray())\n",
    "# tokenizer = Tokenizer(num_words=max_words)\n",
    "# X_train = tokenizer.sequences_to_matrix(sentences_train, mode='binary')\n",
    "# X_validation = tokenizer.sequences_to_matrix(sentences_validation, mode='binary')\n",
    "\n",
    "Y_train = keras.utils.to_categorical(Y_train, 6)\n",
    "Y_validation = keras.utils.to_categorical(Y_validation, 6)\n",
    "Y_test = keras.utils.to_categorical(Y_test, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 10)                16050     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 66        \n",
      "=================================================================\n",
      "Total params: 16,116\n",
      "Trainable params: 16,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "# model.add(layers.Dense(6))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/480 [==============================] - 0s 92us/step\n",
      "Training Accuracy: 1.0000\n",
      "141/141 [==============================] - 0s 148us/step\n",
      "Testing Accuracy:  0.6241\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,epochs=100,verbose=False,validation_data=(X_validation, Y_validation),batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, Y_train, verbose=True)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=True)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
